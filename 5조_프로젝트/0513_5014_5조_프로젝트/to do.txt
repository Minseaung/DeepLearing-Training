2021-05-13~05-14 모듈 5 실기

5조
- 김민정
- 노정민
- 임한희
- 정희석

Condition: Tensoflow 또는 sklearn 라이브러리를 이용
1. Dataset 구성: creditcard.csv 파일 제공
2. 속성들에 대하여 전처리 되어진 파일로 구성되어 있음⁠
3. from sklearn.preprocessing import StandardScaler 를 활용하여 
4. Amount 을 정규화 하여 normalAmount column 추가하여라⁠
5. Class 를 종속변수로 나머지를 독립변수로 사용하여라
6. Train set : Test set  = 85:15
7. ANN 또는 DNN 을 활용하여라
8. Batch size 와 Epoch 에를 각조가 별개로 정하고 이에 대하여 결과또한 발표에 추가하여라.
9. 학습 및 예측 결과에 대하여 각 조별로 임의로 발표 및 자료 제출

kaggle's creditcard.csv data content

The dataset contains transactions made by credit cards in September 2013 by European cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. 
The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.
2013년 9월에 조사한 2일 간 발생한 전체 284,807건의 신용카드 사용건 중 492건의 부정 사용이 존재했다. 
이는 0.172%의 비율로 매우 불균형한 데이터 이다. 

It contains only numerical input variables which are the result of a PCA transformation. 
Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. 
Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. 
V1~V28의 특징은 은 주성분 분석의 결과에 의한 데이터들로 원래의 데이터 특징은 제공할 수 없다.

Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. 


The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. 
-> to do: Amount 을 정규화 하여 normalAmount column 추가하여라⁠
transaction Amount?

Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
부정사용 : 1(492) 정상사용 : 0(284,315)

Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). 
해당 클래스(부정사용 데이터)의 불균형 비율을 가졌기 때문에 정확도 측정을 위해서 AUPRC를 사용해 보는 것을 추천한다.

Confusion matrix accuracy is not meaningful for unbalanced classification.
#합성곱 행렬 정확도는(CNN 예측을 말하는 듯) 불 균형한 데이터의 분류에는 큰 도움은 안된다. -> 즉 CNN 쓰지않는 것 추천한다.

Total column 284,807

#0 : Time Number of seconds elapsed between this transaction and the first transaction in the dataset
4. Amount 을 정규화 하여 normalAmount column 추가하여라⁠
#1 ~ 28 Vn : may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28)
=> 표준화 (평균 -> 0, 표준편차 -> 1)
#29 Amount : transaction amount -> 0 : 275,992 1 : 6010
#30 Amount 을 정규화 하여 normalAmount column 추가하여라 -> NormalScaler()
#31 Class : 1 for fraudulent transactions, 0 otherwise -> 0 : 284,315 1 : 492 
================================
TO DO
1. read_csv-> fraud : 492, normal : 275992 
2. normalAmount -> Min_Max_Scaler -> 정규화 진행, new_column
3. 일단 대충 아무 모델 하나 짜서 다 때려박기.
============================================
Dense층 ->input_dim = 30
은닉층(Hidden_Layer) - 각자가 
=>
12개
Dropout(0.2)층
============================================
4. V1~V28 outliner 제거 시도
